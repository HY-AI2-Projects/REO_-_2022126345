{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# dataset and transformation\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "# from torchsummary import summary\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  2\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 2\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "# Additional Infos\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(GPU_NUM)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = float(obj.find('xmin').text)\n",
    "    ymin = float(obj.find('ymin').text)\n",
    "    xmax = float(obj.find('xmax').text)\n",
    "    ymax = float(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "adjust_label = 1\n",
    "\n",
    "def generate_label(obj):\n",
    "\n",
    "    if obj.find('name').text == \"with_mask\":\n",
    "\n",
    "        return 1 + adjust_label\n",
    "\n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "\n",
    "        return 2 + adjust_label\n",
    "\n",
    "    return 0 + adjust_label\n",
    "\n",
    "def generate_target(file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        objects = soup.find_all(\"object\")\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        return target\n",
    "\n",
    "def plot_image_from_output(img, annotation):\n",
    "    \n",
    "    img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for idx in range(len(annotation[\"boxes\"])):\n",
    "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n",
    "\n",
    "        if annotation['labels'][idx] == 1 :\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        \n",
    "        elif annotation['labels'][idx] == 2 :\n",
    "            \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "        else :\n",
    "        \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "\n",
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms, path):\n",
    "        '''\n",
    "        path: path to train folder or test folder\n",
    "        '''\n",
    "        # transform module과 img path 경로를 정의\n",
    "        self.transforms = transforms\n",
    "        self.path = path\n",
    "        self.imgs = list(sorted(os.listdir(self.path)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx): #special method\n",
    "        # load images ad masks\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.imgs[idx][:-3] + 'xml'\n",
    "        img_path = os.path.join(self.path, file_image)\n",
    "        \n",
    "        if 'test' in self.path:\n",
    "            label_path = os.path.join(\"data_set/data_set/MV_test_annotations/\", file_label)\n",
    "        else:\n",
    "            label_path = os.path.join(\"data_set/data_set/MV_train_annotations/\", file_label)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        #Generate Label\n",
    "        target = generate_target(label_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)\n",
    "\n",
    "data_transform = transforms.Compose([  # transforms.Compose : list 내의 작업을 연달아 할 수 있게 호출하는 클래스\n",
    "        transforms.ToTensor() # ToTensor : numpy 이미지에서 torch 이미지로 변경\n",
    "    ])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = MaskDataset(data_transform, 'data_set/data_set/MV_train_images/')\n",
    "test_dataset = MaskDataset(data_transform, 'data_set/data_set/MV_test_images/')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=collate_fn)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import time\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "\n",
    "\n",
    "# ResNet-101 백본을 불러오기\n",
    "backbone = torchvision.models.resnet101(pretrained=True).to(device)\n",
    "# ResNet의 마지막 fully connected layer를 제거하여 백본을 만듭니다.\n",
    "backbone = torch.nn.Sequential(*list(backbone.children())[:-2])\n",
    "# 새로운 Convolutional 레이어와 ReLU 활성화 함수를 추가\n",
    "conv6 = nn.Conv2d(2048, 256, kernel_size=3, dilation=6, padding=6)\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "# 새로운 백본을 만들기 (ResNet-101의 마지막 4개 레이어 + 새로운 레이어)\n",
    "backbone = nn.Sequential(\n",
    "    *list(backbone.children()),  # ResNet의 마지막 4개 레이어\n",
    "    conv6,  # 새로운 Convolutional 레이어\n",
    "    relu  # ReLU 활성화 함수\n",
    ")\n",
    "\n",
    "\n",
    "backbone_out = 256\n",
    "backbone.out_channels = backbone_out\n",
    "\n",
    "anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(sizes=((128),),aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "resolution = 7\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=resolution, sampling_ratio=2)\n",
    "\n",
    "box_head = torchvision.models.detection.faster_rcnn.TwoMLPHead(in_channels= backbone_out*(resolution**2),representation_size=1024) \n",
    "box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(1024,2) #21개 class\n",
    "\n",
    "model2 = torchvision.models.detection.FasterRCNN(backbone, num_classes=None,\n",
    "                   min_size = 576, max_size = 1000,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   rpn_pre_nms_top_n_train = 6000, rpn_pre_nms_top_n_test = 6000,\n",
    "                   rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=300,\n",
    "                   rpn_nms_thresh=0.7,rpn_fg_iou_thresh=0.7,  rpn_bg_iou_thresh=0.3,\n",
    "                   rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "                   box_roi_pool=roi_pooler, box_head = box_head, box_predictor = box_predictor,\n",
    "                   box_score_thresh=0.05, box_nms_thresh=0.7,box_detections_per_img=300,\n",
    "                   box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                   box_batch_size_per_image=128, box_positive_fraction=0.25\n",
    "                 ).to(device)\n",
    "#roi head 있으면 num_class = None으로 함\n",
    "\n",
    "# for param in model.rpn.parameters():\n",
    "#   torch.nn.init.normal_(param,mean = 0.0, std=0.01)\n",
    "\n",
    "# for name, param in model.roi_heads.named_parameters():\n",
    "#     if \"bbox_pred\" in name:\n",
    "#         torch.nn.init.normal_(param,mean = 0.0, std=0.001)\n",
    "#     elif \"weight\" in name:\n",
    "#         torch.nn.init.normal_(param,mean = 0.0, std=0.01)\n",
    "#     if \"bias\" in name:\n",
    "#         torch.nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(576,), max_size=1000, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (9): ReLU(inplace=True)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 저장된 모델 파라미터 파일 경로\n",
    "model_path = 'Pmodel_.pth'\n",
    "\n",
    "# 저장된 모델 파라미터 불러오기\n",
    "model1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 이제 model 변수를 사용하여 예측을 수행하거나 추가 학습을 진행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.backbone[0] = nn.AdaptiveAvgPool2d(output_size=(36, 48))\n",
    "model2.backbone[1] = nn.Conv2d(3, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
    "del model2.backbone[6]\n",
    "del model2.backbone[5]\n",
    "del model2.backbone[4]\n",
    "del model2.backbone[3]\n",
    "del model2.backbone[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 550\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print('----------------------train start--------------------------')\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations) \n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        epoch_loss += losses\n",
    "    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list].cpu()\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list].cpu()\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list].cpu()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[666.8348, 210.2168, 695.9542, 293.7704],\n",
      "        [278.4483, 434.6573, 331.0816, 573.3550],\n",
      "        [573.1374, 205.0152, 599.6122, 284.7596],\n",
      "        [447.0145, 304.0365, 481.5672, 415.4805],\n",
      "        [637.6471, 308.3922, 667.3739, 405.0156],\n",
      "        [196.2499, 150.5820, 219.5780, 215.6305]]), 'labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9998, 0.9997, 0.9997, 0.9997, 0.9996, 0.9996])}, {'boxes': tensor([[283.8163, 438.5364, 335.2435, 572.6788],\n",
      "        [636.6741, 307.5641, 667.8004, 405.7942],\n",
      "        [674.8387, 213.6339, 701.7568, 294.1721],\n",
      "        [563.8549, 201.5002, 591.1240, 282.2188],\n",
      "        [187.2625, 147.5019, 213.8300, 213.5357],\n",
      "        [459.8701, 308.2160, 500.4918, 410.3006],\n",
      "        [462.9914, 303.3878, 494.4921, 402.5475],\n",
      "        [190.2673, 152.3691, 219.7935, 216.1737],\n",
      "        [464.1844, 288.0768, 499.4271, 398.7256]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9998, 0.9997, 0.9995, 0.9990, 0.9989, 0.9988, 0.9658, 0.9482, 0.8327])}, {'boxes': tensor([[286.4862, 439.4594, 340.3503, 572.1158],\n",
      "        [680.7128, 215.7963, 707.5346, 300.5748],\n",
      "        [466.1191, 302.7197, 501.7260, 405.4552],\n",
      "        [637.1335, 307.8510, 667.3807, 405.1186],\n",
      "        [181.6702, 148.2006, 206.4183, 215.2758],\n",
      "        [556.6383, 202.2584, 584.2631, 281.3857],\n",
      "        [185.8257, 144.6919, 212.3701, 211.9980]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9996, 0.9995, 0.9994, 0.9991, 0.9987, 0.9963])}, {'boxes': tensor([[477.2844, 307.2033, 510.9919, 405.2808],\n",
      "        [551.3763, 200.8972, 579.2165, 279.3893],\n",
      "        [297.1675, 437.9705, 348.0059, 566.5123],\n",
      "        [636.6569, 307.3233, 668.2446, 405.3115],\n",
      "        [686.4241, 218.8109, 713.6187, 301.4655],\n",
      "        [177.1216, 146.8944, 202.4806, 213.5513],\n",
      "        [548.9667, 205.6171, 573.6444, 282.2592]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9998, 0.9997, 0.9995, 0.9995, 0.9992, 0.9991, 0.9981])}, {'boxes': tensor([[307.0948, 432.9788, 360.4583, 566.7856],\n",
      "        [167.5519, 145.1957, 194.0968, 212.3929],\n",
      "        [638.9715, 307.9463, 667.8398, 405.3341],\n",
      "        [482.1465, 301.4474, 515.5752, 399.9311],\n",
      "        [546.6528, 199.1042, 575.3058, 272.1358],\n",
      "        [690.5524, 222.2709, 717.1130, 307.0441],\n",
      "        [692.6745, 215.0088, 721.6638, 301.4263],\n",
      "        [546.6791, 191.8812, 580.1728, 263.5211],\n",
      "        [486.8658, 301.1711, 521.9459, 403.6163]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9997, 0.9997, 0.9995, 0.9995, 0.9992, 0.9985, 0.9874, 0.9795, 0.5409])}, {'boxes': tensor([[168.5535, 145.7984, 194.6391, 214.7458],\n",
      "        [309.0022, 434.5143, 362.6069, 565.5640],\n",
      "        [489.1387, 301.0676, 523.4836, 404.5196],\n",
      "        [636.7474, 308.8417, 667.6283, 403.6724],\n",
      "        [539.6299, 198.6289, 565.7126, 275.8832],\n",
      "        [691.8514, 219.4339, 720.5649, 307.0297]]), 'labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9997, 0.9996, 0.9995, 0.9995, 0.9993, 0.9987])}, {'boxes': tensor([[310.6293, 434.8607, 362.4201, 563.9227],\n",
      "        [536.6405, 194.5136, 564.8109, 273.8385],\n",
      "        [160.9673, 143.6960, 187.2566, 212.8682],\n",
      "        [507.5297, 300.1227, 541.8331, 406.7430],\n",
      "        [636.2126, 308.0771, 667.1780, 399.2458],\n",
      "        [701.7724, 222.2190, 729.0539, 310.9251],\n",
      "        [314.6032, 405.7999, 365.3887, 573.4778]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9996, 0.9996, 0.9995, 0.9991, 0.9988, 0.9859])}, {'boxes': tensor([[315.3070, 436.1338, 364.9128, 563.7292],\n",
      "        [637.2143, 308.4581, 668.0585, 403.2282],\n",
      "        [527.2295, 197.2708, 556.0739, 276.5421],\n",
      "        [157.0223, 142.6040, 183.6929, 215.0326],\n",
      "        [512.8486, 298.3381, 547.1240, 402.1467],\n",
      "        [704.7787, 227.0445, 730.9326, 313.7422]]), 'labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9995, 0.9994, 0.9993, 0.9993, 0.9991])}, {'boxes': tensor([[710.2419, 231.7054, 737.7281, 318.3599],\n",
      "        [  5.9075, 178.4930,  30.5627, 248.0285],\n",
      "        [527.8894, 296.6893, 564.0669, 398.8091],\n",
      "        [636.7661, 306.9033, 668.7475, 404.0128],\n",
      "        [340.1097, 440.9425, 389.7248, 562.3839],\n",
      "        [517.1330, 191.8588, 546.0963, 269.1060],\n",
      "        [152.8445, 142.2383, 179.0655, 214.0401],\n",
      "        [  6.1272, 190.3003,  28.3446, 262.0397],\n",
      "        [156.8067, 141.4942, 188.1748, 212.3193]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9998, 0.9998, 0.9995, 0.9994, 0.9994, 0.9992, 0.9991, 0.6769, 0.5330])}, {'boxes': tensor([[717.8428, 230.2955, 747.4650, 320.0606],\n",
      "        [  5.5901, 180.1236,  30.2088, 247.8783],\n",
      "        [335.2275, 434.5607, 384.7819, 554.7537],\n",
      "        [638.1444, 305.3289, 669.8324, 402.0300],\n",
      "        [511.9856, 191.0995, 540.5653, 269.2185],\n",
      "        [538.7795, 293.5870, 573.6160, 398.2974],\n",
      "        [149.7240, 144.8225, 174.0216, 214.3640],\n",
      "        [  7.7058, 184.8029,  34.8106, 252.1681],\n",
      "        [721.5595, 239.1837, 751.4200, 324.5208]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9998, 0.9998, 0.9997, 0.9995, 0.9993, 0.9993, 0.9989, 0.9980, 0.9933])}, {'boxes': tensor([[345.5818, 436.2853, 394.5683, 558.1359],\n",
      "        [717.8796, 230.8460, 750.2062, 327.2487],\n",
      "        [547.2392, 291.5421, 581.0165, 392.4940],\n",
      "        [ 10.3347, 181.3069,  35.4833, 252.6830],\n",
      "        [511.5681, 191.2362, 540.0325, 266.8754],\n",
      "        [148.0512, 146.6421, 172.8727, 214.2923],\n",
      "        [638.2360, 304.9337, 668.6607, 398.6990],\n",
      "        [721.2301, 241.1805, 754.4377, 332.5060],\n",
      "        [ 14.5275, 181.6084,  40.1802, 252.5132],\n",
      "        [549.6867, 302.4602, 579.8259, 411.4547]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9998, 0.9998, 0.9998, 0.9995, 0.9995, 0.9991, 0.9990, 0.9773, 0.8288,\n",
      "        0.5713])}, {'boxes': tensor([[346.2838, 442.3599, 399.4115, 563.8604],\n",
      "        [142.4416, 141.1513, 169.8415, 213.0009],\n",
      "        [ 12.6194, 181.5973,  50.5060, 251.6910],\n",
      "        [ 17.4216, 180.1833,  43.5735, 249.7872],\n",
      "        [553.3585, 295.6975, 587.3171, 393.0994],\n",
      "        [722.2894, 233.7132, 752.7108, 329.4006],\n",
      "        [501.9012, 188.0517, 531.7602, 266.6499],\n",
      "        [639.0528, 306.9788, 669.6190, 397.3467],\n",
      "        [556.7213, 302.8334, 594.1442, 396.0340],\n",
      "        [348.7891, 398.9445, 396.0732, 567.4783],\n",
      "        [723.0375, 245.5844, 754.9158, 345.4914]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9996, 0.9996, 0.9995, 0.9995, 0.9992, 0.9990, 0.9987, 0.9974,\n",
      "        0.7357, 0.5476])}, {'boxes': tensor([[349.7022, 438.9577, 393.8070, 556.0427],\n",
      "        [ 27.1653, 182.1240,  52.8248, 253.4397],\n",
      "        [500.2529, 185.8468, 529.1884, 262.6176],\n",
      "        [139.9748, 141.3795, 165.5699, 209.3622],\n",
      "        [563.8929, 290.9734, 596.5280, 386.8703],\n",
      "        [638.1426, 307.9190, 670.8213, 404.3594],\n",
      "        [ 27.6057, 182.1595,  65.7370, 252.7277],\n",
      "        [722.5019, 227.6258, 752.2667, 329.6169]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9996, 0.9995, 0.9994, 0.9993, 0.9992, 0.9991, 0.9983])}, {'boxes': tensor([[361.0590, 440.8621, 411.8929, 562.4348],\n",
      "        [136.8774, 145.6911, 162.2106, 214.4668],\n",
      "        [ 29.6157, 179.3476,  55.1612, 252.6973],\n",
      "        [489.5183, 183.6963, 518.7655, 260.9346],\n",
      "        [566.3048, 295.3406, 599.0045, 391.8343],\n",
      "        [638.0552, 307.2497, 669.8539, 402.2737],\n",
      "        [560.9074, 287.1991, 595.5090, 387.8434],\n",
      "        [364.4669, 404.5382, 408.2974, 560.2683],\n",
      "        [356.9363, 428.8698, 404.2072, 555.4727]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9998, 0.9998, 0.9996, 0.9996, 0.9993, 0.9992, 0.9697, 0.8414])}, {'boxes': tensor([[ 32.3576, 177.0712,  57.9801, 249.8265],\n",
      "        [137.8117, 143.4004, 163.8382, 211.8841],\n",
      "        [486.1991, 185.5357, 514.9327, 259.1358],\n",
      "        [569.7260, 285.0210, 603.2684, 383.3842],\n",
      "        [359.6613, 432.8479, 404.9802, 547.1602],\n",
      "        [636.4357, 308.8184, 670.1251, 398.6431],\n",
      "        [ 37.0632, 183.9709,  60.0628, 252.6741],\n",
      "        [ 39.6245, 177.8348,  70.4813, 248.1862]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9997, 0.9995, 0.9992, 0.9991, 0.9988, 0.9984, 0.9668, 0.7338])}, {'boxes': tensor([[ 39.2219, 176.2800,  65.8273, 250.3475],\n",
      "        [133.1009, 141.2581, 160.3220, 213.0380],\n",
      "        [366.3799, 434.4262, 413.5918, 556.7273],\n",
      "        [478.2671, 182.2224, 506.9247, 256.0047],\n",
      "        [578.6835, 280.7302, 609.6410, 376.9205],\n",
      "        [636.0367, 307.6381, 665.9542, 399.9545]]), 'labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9999, 0.9997, 0.9996, 0.9995, 0.9994, 0.9984])}]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "with torch.no_grad(): \n",
    "    # 테스트셋 배치사이즈= 2\n",
    "    for imgs, annotations in test_data_loader:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "\n",
    "        pred = make_prediction(model, imgs, 0.5)\n",
    "        print(pred)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target :  tensor([1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAD8CAYAAAARze3ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAsxUlEQVR4nO3deZRc1X3g8e/v3lfVrW7tK0JqhAABZhUgYwi2MeB9kycz9sgzdkgGH3IS4jWJDeOT8UlOOGNPzjhx5sRJcOKEHC+YeAnYkzHG2BjHwQYJgy12DBgJydrQ1mu9d+9v/nivukutalW31NVVJf0+nKKqbr+q+nWp+ld3e/eKqmKMMWZirtUBGGNMu7NEaYwxDViiNMaYBixRGmNMA5YojTGmAUuUxhjTQNMSpYi8UUSeFJFnROTGZr2OMcY0mzRjHqWIeOAp4HXAVuBB4N2q+ti0v5gxxjRZs2qUlwLPqOqzqloBbgPWN+m1jDGmqZImPe8KYEvN/a3AKyY6eNGiRdrX19ekUIwxprFHHnlkt6ouqfezZiVKqVN2SBtfRK4HrgdYuXIl3/3ud5sUijHGNLZkyZJfTvSzZjW9twK1VcSVwLbaA1T1FlVdp6rrFi1a1KQwjDHm2DUrUT4IrBGR1SJSBjYAdzbptYwxpqma0vRW1UxEfg+4C/DA51X10Wa8ljHGNFuz+ihR1X8F/rVZz2+MMTPFzswxxpgGLFEaY0wDliiNMaYBS5TGGNOAJUpjjGnAEqUxxjRgidIYYxqwRGmMMQ1YojTGmAYsURpjTAOWKI0xpgFLlMYY04AlSmOMacASpTHGNGCJ0hhjGrBEaYwxDViiNMaYBixRGmNMA5YojTGmAUuUxhjTgCVKY4xpwBKlMcY0YInSGGMasERpjDENWKI0xpgGLFEaY0wDliiNMaYBS5TGGNOAJUpjjGmgYaIUkc+LyE4R2VxTtlBE7haRp4vrBTU/u0lEnhGRJ0XkDc0K3BhjZspkapT/CLxxXNmNwD2quga4p7iPiJwDbADOLR7zWRHx0xatMca0QMNEqar3AS+NK14P3FrcvhV4R035bao6oqrPAc8Al05PqMYY0xpH20e5TFW3AxTXS4vyFcCWmuO2FmWHEZHrRWSjiGzcs2fPUYZhjDHNN92DOVKnTOsdqKq3qOo6VV23aNGiaQ7DGGOmz9Emyh0ishyguN5ZlG8F+mqOWwlsO/rwjDGm9Y42Ud4JXFvcvha4o6Z8g4h0ichqYA3wwLGFaIwxrZU0OkBEvgy8BlgsIluBTwCfBG4XkeuAF4B3AqjqoyJyO/AYkAE3qGpoUuzGGDMjGiZKVX33BD+6ZoLjbwZuPpagjDGmndiZOcYY04AlSmOMacASpTHGNGCJ0hhjGmg4mGOO3q9Q9rc6iDbQBawCpO75CMa0P0uUTbQH2N7qINrAbOAU6p+2ZUwnsKa3McY0YInSGGMasERpjDENWKI0xpgGbDCniZaqknjozjwvW3cRPVu2NHzMQF8f33xo0wxEZzrZ7mmaUTEfWGTDbA1ZomyikyPM946e6OnZsoUvvLSd7jRhOIEk1l2mkw1LltYtN6bWbvI1DY/VKsBWg23Mmt5NFMSDJgz7/G0uh4BKhmhscWTGmKmwGmUT7XBKv2YgsAxYpWUAdkc4aK0dYzqGJcom+pUEXgqR6AIvB06VLpRIiBkHpX7T2xjTfqzp3URBkrzpneQ79naFgEiKs6a3MR3FapRNlMSI4EDzRBklQ0UJzqMuoO4ASWUxIiOkTplffG+dMu559hcX0xqK8iugMoXHLAO6x40mK8ouYKjO8YuB3jqjz7tRBuocf3AKsQDMKy7jzZ/i85yoLFE2kRBInYwmSpVIKfNEcWQMkfkdeF2OZxgksqhIlGeO+4N51hbXaCkFfgn0T+Exc4DuOuUvkq8BMF430FunfAfTs17AQuB0mwZ01CxRtoAjIzDEsPZTSjIkgosJDt/q0IwxdVgfZStIhpNhvA5T0gplDSTR4dS+8Y1pR1ajbIEoEa+BbQ9vZuf2YTSOsPKCdbB0WatDM8bUYYmyBaI4XKbc+Zkv8/Nv/ZLoM/7wb/+GC97y1laHZoypo20SpYuHNzuDC0QRfHBs9wGNgsOxWLR9Aj8Cr4JTxRdTJrcBwcFPH9jI88/9nPSlDKcpmmZsvvdelkXlEkA1f4CINcU71W5giMPnyo5McPxLQFbn+C7g5DrH74e6o+GmOdoi3yhQScIhZaJQikoUDzHhqWSEjAQRuIw2CbwhIbiIFgnvCRUyUe76+r9wz63/BGUh9SkuOu7+xy/w4oMP8WHyBFlNlqYzvTDF418sLuOdA5xRZ7T6iQmmDZnmaIt841C6s0MnYQsQtYSKp+KlSJoBFQV1dMLGAhUfSSWODtKoFIthSEJwDhnJSKIgAsELsTgnXFWtNmlMG2mLRFlB2CKHhnLWJRdTrlmW7O11Hhf6+tj70ENNju7oLY5CV/RoMbkgSp4A33Xd+3jVukv50498BB0cICp8+MY/4OqrroLXvX60RmnJ0pj20BaJchB42B86U+n8LVu4bdd2yjGSEDnPz6UbJVHFFQtNLF6ypDUBT9LJWqIcPBWfz48MLqJ45p26msSVSGbPpZJmeFHOPfscLrno4tHHWpI8sbzt4kvoncR6pZBXELA1S2dUWyRKB8zKDi/PpISTQJpkdFeUkgt4zVARtAOa3ioBh1IO1fuKj0qGp7R8Bf/7K19msYMF4li9fMWEa1Sa41/vli3ctmvnYeXnACeP+6y3ewXheNQWiVJRRnx6WHkSwWkkVc9uqSDiEacsiZ1xDstIAqKKq3a/KpSDIBFGuhIWnH06fT6hj4RSjETqL5bRC3Tycr6z6IQe5dbbWOddWkb9Ue85TO0zMfsoYzK5holSRPqAfwJOAiJwi6p+RkQWAl8BTgWeB96lqnuLx9wEXAcE4AOqeleDV8FrnZOEJJK5DAm9PJYchDAfT8Y8hFl1plK0Gx8dPo4NPEXnSJ0wK2QgkUoS8QF6Mki9EibI/ssQbCr68e8Wd/jfwJoYOb/OsSsQVjQ/JFOYzCmMGfD7qvoy8pk5N4jIOcCNwD2quga4p7hP8bMNwLnAG4HPishRVgCFPJcLaAnQfN5QhygFxasSi5hVAiNJYKikZAKiguAQ9XRlniTYGaXGtKOGf5mqul1VHypuHwQeB1YA64Fbi8NuBd5R3F4P3KaqI6r6HPAMcOlRRSfVjksFLRfXnZMoAYIoWtR+T6k4Tkkds2N1MrpjvygvlCIvlJR+11m/mzEniin1UYrIqcBFwE+AZaq6HfJkKiLVLpMVwI9rHra1KBv/XNcD1wMsXrmy7utF2Q+xjHOCxhJCoJN6u6KAeh3dI+c87cGp42lShkKGimOHZOyQEaLAGXhWtThmY8zhJt3WE5HZwNeAD6nqgSMdWqfssKqSqt6iqutUdd2cRfX3gct0D5nuBzcIRJAwlZBbzmv+ZqjkibI75KP21bN1RCGIY9gnxWronfO7GXMimVSNUkRK5Enyi6r69aJ4h4gsL2qTy4Hq3IatQF/Nw1eSn+Z8BIqooghRGE21ZRkkdYHAXHABpz4/ta9D+il9zGuVWZH/RpJI6oQRB1mAkgouKmUVSgFKNnfSjPOEKD9QD5KCehT4T8CLCCs6YEDzeNGwCiP5zOe/Bx5X1U/X/OhO4Nri9rXAHTXlG0SkS0RWA2uABxq9TqIRr0Lmhazoqws6zN4XHmPnz7az64VfEDQpkuThU4naUebyRFlNfyqQqFIO+SzQIOTzQVVInZBanjTjfM7Bb0qJ33SR33LCb7n8Q3JPB7WsjgeTqVFeAbwX+LmIPFyU/Xfgk8DtInId+RoA7wRQ1UdF5HbgMfIR8xtUNRz2rOMoQnXdWl8M2OwrwWfe/2cc2DjE4rWn8kff+BdiCTqmn1LGrkJfH0uKicJHmi4c+vqO8FNzohFN8q6n2INKQIo/EkfDPykzjRomSlX9NybOTNdM8JibgZunEojiiCII+dkrALvvf4p4cIiQ7iPbsZstP/gRSy88G5YsmMpTt4Xac9KfQNla55gzgFM75UvAzAgfgdIeYnYa6vuL2R8gZLTJ+SInhDapvwtRJD8vRZVSUfqZd32cF7fsZF93ha0vPMcf/ca1PP3gA2BbJpgTRDmO0ONeLFadShlrpliNcia1SaIEFTe6bqML+YdgeFDIhh0QCBIJUgEyxGpd5gTRrRV63bZ862NXIUh1yT7bG34mtUXdPSHfczgCUZWe4sPw6++/ga/81edIgrJs+Um858O/ywXnnt0+2f0ozaL+Hsv1tjc1J5J8danTVFkoikRhnlYIsZ9KlvJsrLC1WDggdsRqB82no1PwFFTYJ4pTyZc0nMbXaYtE2auwjogqJNGjxYfg0jddxZf/5m8ZFseKxUv5nf/6XnrxBO3s1vcpxcWYWkJGkDIfiilvkwwJs8iSyBcf3Mze+/6aIPsYuOBqAJLQU3Pm2onNRUellCIVz0+6oLviSJMKMo0LfLdFohwb7Xa4KPiiYPFpZ/Cnn7uFqCkXz1+G8135PMoOnz5mXQfmSFTz/wlKxZV55L5N/NOnvo6T/Zz2G/kxTvaTrytl6lGmd25MWyRKyDcXy0TYlyiZCGcBsmAhZ19zFd4FLtBeRJQoEaJ1ZJvjl0i+HcimjZv42ne+zK4XDtAzMkCapOx4+PsALDhwAOae2IlywcUX48ctdvyeBo8Z6Ovjm0ex6HHbJMpSdAwmwtOhn70lx1lAiocExCmaCU4jgdDxfZTGHFExj/jpZ57m87f8A4LSpYIEhQcfAeDsoTLMbWWQree3bGHXzl34mqb33V1Kd8VTSSq4Ok3vDUuObmXXtsk5+VmJjqHEM5jkfZTlkHfSBhdxBJKYWb+MOe5VV5s6+eSTOe21r4PZCxjoKjHU5Vh6Zr6AjO+x7puZ1BaJMgP2CwwpuODoTvOwujMt9sXOzwX3qqhoflpg0U85qMpBlIEItpOC6WQvA85RZZ44RByvfPWruOqzn6Z36UnMzuYzJ+3l8vW/DsD8np7WBjvDAvnfee0F8v22tLreaz7+Xdya3i+Stmh6DwE/8hmQLz1WXZZRJZJ6mJ0JqUtwmSPiiR66R/Ja56Nk7PGOOcFzseRTb4zpRN+InqLDCVUHLqPSNYvut72TcMU2HMq3L7wAgP9LD/95gq1DjkeDwIMcOo67GthMZJ1TfEyoiMcxgiColoAwbemyLRKlUjvdZ+y7YKAcqPhIiJFQfFOUs/xYKfpxMlFGfETKEENn7PdtTD2l2s9uMZc4+B78h36boXLGUBky6QIgicKJNpVSOXzCiyL46FHxZE5IohJcIDrNT/88nqYHTWRRpoSo9AYh8bFYiSefHlTdXmFBlu+anUTF1yzRZszxoOIcO3oSFh2cjQah4vM++sGuLF+a6gSXCexxHgRSlEWpEFFSjRwQN235oK0T5Xn0kqCIj3hV0iJZ5jXQPFGekfSiOByCo0LHT7I0psaC4YCfpaSljOEuQSVfYjC4jGIprRPaMMpjYRCvSjfCmtIcnCghRDZqOm2dE22bKAf6+uhbduT5D6Gvj1lZxGk+wJN6S5Lm+JI6hyiIVOhKlVg917t9/3RnVEQY9CU8ESUwO1XUZ8QQEDd9087b4t12wPgxvHsf2sS5ztEbwUUlFLXormJ2UMXnLQ+vEUWo+GIf8BmO3ZwYuqHpQyf9dVpDs3yJU1Aop2iMEPJa5Ny0XKwmNDkVdErLXZeAcof0Y5WCBxHURwTIXCQS6VVHptUxjdpxkLHCHhVUdHSdz4m0RaLsBV5Rp9zF6javxfeC5gmyanQygCilE2cA0MwwAS6g+Z06D9Yp+y+S8rGKIFricac8n1QAuDxWyDvlJ+dF4LkpxLICOGsKx7eKAJkHFxN60hEGSgk9OgKUuTwGKj7fYibRvPYJIC7i0jKhnPFrI8JIOVLOjtyN0RaJUgB/pG+vaqKUw4rHfmZMk8gMnZ2fcfgf6+wssiAVUueZJUK5KJ/qPnTK1GrEndWJFVEXUAUt7YdKPhqeefKxCxWS6HBFrVF07N/TkU8eaNRr1xaJ0hgD3XWa0sPesVscmQgjCC7mGbKzElnzCFpsIzNCkIQ9g08wKzkdh0dxeHVkCP0JBBHmcHQrj1miNKZNvCo7/C/4SVdhIxGJQiYOX/TCWytqjKgn+IyQdTMiL6KcASokUfAKB73yRBjgYOI4DQiqU56CaonSmDYRk8NrlCMCB8UDQikKSbFwVievxzqtFPI0pgR1xGQvIeYLiORjFw4tOfqd40CpSI9WozSmM4W+PpYuPvmw8qvrHNvf10eojmSeIATo4vBfuYxQUQEcqpEf3Hc/v/c/b8WlJV555WX86Y3/gyQKvbh8/2igWyGRfFxkGCXVxnP3LVEa0wb2bnqobvmjKNvHlanA2SdQkoR8ZsxldcrXivJTN4gMdpFVnmPXi3P52eObKA3N5dSTl7F3uJ/yrC7WSUKI+XDW5QrRR6RS4v6uIVyakLkjv6E27dCYdiCTv5yIrW5BSMZdABKEGCNbH32C9791A3d98W5czHcf+u59P+Dqa67muaeeoRwdvWleL5wVEsqxmHsZIxUXGXZHXgzcapTGmI6lgDhP6B9k57PbeCkO4yJkmjIwNERlyxADlQr7k3yX1znAviQhOCFVmJcKGWG0tjkRS5TGmI5VnR/as3AuL7/iSp5+eBMH9u4DAictXsTacy8kmTObx8MA/YljFfCzMIjPlBLC6tJsvEDSIFFa09sY08GEFFj+stP5k89/jjMuegUx6YZEuezXLuO2L3yRU08/nYOJY18prxceSEr0+4T+BOZkyuwsMC898s4JVqM0pg1kaN0zZ45Uz6m3udZEFgOXki828+1ic63qLgH1php11FKX3jEkkSgJ5115JYuXLsHHQS5cdxEjDkIEj6M7zX+r3iyfjaqliM+U4ANpgy1mLFEa0waeAnbVKT/SEIPfsoXduw591LMo9VLnchH6EPoWL+ZyFaIoJc2TZVYnK3ZKU1NUSTIlUiIIvP76a3FFuTjhPo35Tgnq8EXz+hKJeeILCbgs32KmwVdDw/dDRLpF5AEReUREHhWRPy7KF4rI3SLydHG9oOYxN4nIMyLypIi84RjeB2NOCBmQ1rlMda2XMMHzdGXC/GJFmVnR0YXgXH4pc/gl6ZSxdalOBqhOCRCiCMHlpy5WJP/9kbHpAiVRypL/nnlx47P5J/PFMQJcraoXAmuBN4rIZcCNwD2quga4p7iPiJwDbADOBd4IfFZEOqomb8zxpuIc+4szU4a9IMWqGrZI+uQ0TJSa6y/uloqLAuuBW4vyW4F3FLfXA7ep6oiqPgc8Q949Yoxpke1U2MRBAHbEfM/rUhAazLM2hUl1RYiIF5GHgZ3A3ar6E2CZqm4HKK6rO4uvgEO6SbYWZcaYFhkR4aWiRjnkXb7UmIqdMz5JkxrMUdUArBWR+cA3ROS8Ixxe760/7HtLRK4HrgdYuXLlZMIwxlTpuOuCo/4fdTk4uoqdS0sxT5SZg9QJ5Xjs1UrVYiHtYinxrFhVHNF8Unixongyuri4jC3I3RTjX1dG99k6mtec0qi3qu4TkXvJ+x53iMhyVd0uIsvJa5uQ1yD7ah62EthW57luAW4BWLt2rTUAjJmCscGHQ//sVyHUrXb4sf1jVqJkPl8pffp2BpBiu4XAAfVs8tAVIEqG4nHqmeMy1qXCcKL46EGauy2BipLEhJhkXDEiVMr56x1xkfAJNEyUIrIESIskOQt4LfAp4E7gWuCTxfUdxUPuBL4kIp8GTgbWAA9MOTJjzISi5BOHoqscUu6o358m6pBiCkxSu334NMWz8JKLRud0LgFOP8Kxs/v62LfxkeYvfiT51tZCvufRsfyuk6lRLgduLUauHXC7qn5LRO4HbheR64AXgHcCqOqjInI78Bj5rIcbiqa7MWaaSLHSuWuw18vYA2jq5Ei/ZQsv7dhNcCn7QsL9ZehOIfoKkQQfE+b6jCsqMHf5suYF0iQNE6Wq/gy4qE75HuCaCR5zM3DzMUdnjKnrFy7/o/xhMrl62SmaN8vN0bEzc4zpQGnRjB7WydUoMyLN33D3+GWJ0pg2MFHf4kSpTYqTG2WSu3UrY6dDhqb2DuZD2dXzyAUZ3elQamKIaLFG2uSeVY/iXHhHvqPvdHw9WKI0pg2sAU6rU/409c8B12LEWCVC7AXXz2jWqTM5ciuwA2E98ONpifhQ64HMCV1ZF5lXSjpCdJ6lWYnViTAr8+zC8SM/xFuBZ4Ny2hSyzyDwCIfOhlpflNVLhF0Ia6PHS4QgBA8+Hn0nrSVKY9pA1wRVq2SC2p+M7lHtyDeyrp4lXL+alhUXgKFjinRiSSwW2XCQuUDF5TtHzguOBMU5Zb/L09re0XlJk6tSRvK4x78bwxMcL8AsqiP8RS23eK2mz6M0xrSJ4lxtVQ/JbiRbRr78Q027d4Y5VaIoiQZmp5ERn1GOHqeOKIrXyOw0T5BlDYA/pjk7A319bFiytPGBNUJfX+OD6rBEaUwH0iLDqCr4F5F0OTgtEmhrEmXqFKfKfDzn+9mAo4SS+gwXlXk4znOzATiDbsbquEfnm8W6mvV0k29GNl2rIHXKsnPGmBoyWmtMGdHngQiS0tI9bAWii3RFZUkKSyvK3JDHM1pe5MZ5QTtqspLVKI3pQKNjwJKhbg9KQCQFKVUPGD2yOrbTaM3FY+UDpF7zASYRMp+fX90VoOIhSEQkj2GoFPHaOTtKWqI0pgO5mA/eeI0M7+tneOevKPk99MxdQTJ3ISiUQ0QkY7CcEUQoheb+uf87PUA/gseVBsnCXOYzyDnO50laIBTrujntrOUdrOltTAdKimpif6nCN//qHj561X/kg9es5+6v3kbmMoZKkeGyI4gjiY5kBlboPShwUCIHRNnnBul3w8VeNmPbklerkI7OqU2CJUpjOpJo3vTe8ejjZNsz3MHdxL0H2f+L59ix+WfEOMJwEql4j8TSaA20qTHJMEg+8l1hLxXdjYgiSD7o1MGs6W1MB8p8Xh/7kze9m3J3pN+Bj4Hv/N2t/Ojb9/Dnd/8/dNEiFE/qqrvCNDlZuWEUV4zI78drSinOAxjtm+xUliiN6UBpkXj84AjDwxEtQUwVVdAskgTFBSVKJHMRpViSspkkA+3BqcfpIDI4RKwsZ7Ar0NXVhXOuYxOmNb2N6UChmHC+4fdvoKd3Cb6iaIRzXvdq1v/B71KZ242KUoqBkgbcDCyIoUTQEj56fDrMn/3+J3jvG9Zz9dVXs3nz5uKYPFuralOb46N9otPEapTGdKCemE9IfPUNv803v/ot/P7dqAu89pWv4n3vvRbR/Pzu7d7lSUObnygvi91ICOwe2M5L6SwObj3Arh1bkYrjp9u2s29VH90LFvAW4JcBVk2h27QHuGQKsUy0yMjRskRpTAfSYpqNn9XLR//4j1m2f4QogTPXns98dXiN7HZCEMHP0A5iCzQBddz08U/wwwd/wMDAIMRA5pRPfPgjXPTqq/jDv/o/AOwow6op5G6PML85YU+KJUpjOlAocl/iE65605tZFRIckSAKk1x6bbr1eweuxMGhCr/65Y5ignsCZAzu20dl/0HKxZk6I9O1/tkMsURpTAcanbAdHdupsF9GAOUkSZjaMhHT55kwTNDIGa94Bdekw3zv7u8j0dEdM85/3ZWcdeXlHOjKM/yc6lY/HTK2Y4nSmA5UPVPa4TjolL1FU3x2hKUtqqntEAiJ8Pb3/TfefPkruO97PyBFyJJe3vX+D3DS5Zcy7PK4u0LMV9XtkExpidKYDpQVIxVOIargoqcUlZK6pm8DO2FMkpAWK5qvWrSYP/yd32WPT9hdLjFv+SpmpSWSIqEPlLTuAsPtyhKlMW1sFXBSnfLnRnOM4FQRhXIQtrnITpff7lfwREpBUXGjybVZXh49GRmK47nlJ3H1xz5Gvw/0q8OrIxAZW62jc5IkWKI0pq3NRphdp3xnLOYjogSXrwWZecWp4GM+JShzWlzAx+avHrRYIWhkX3TsLAvd+S4M+V454xYTdh1UmwRLlMZ0pKRoXavkKwj5EIgCTh2iUlMeUVGiuNFV0c3UWaI0pgNVFwNSiZxEwhLtRjSyTQI7vSJETtKEpZogGtkugZ2tDbmjWaI0poNFicyLwimpQ1AOJsL2JOKJzAnQlzkcykAi7HSdvYJPK1ld3JgONNb0zne8VgmMJAGVfOXwCCCKSqCSBCq+g2Z3tyGrURrTBn6Jsr9OeZxgR8UBn9dxgjhUhMwpg6VAeURI1JH6iCJEcaQusvA7P2TBHT8E4Nw77iP99VfTnZYRjUR/bJt8Vb20GVQSUhVWFwNIKgn1RriXPiuwuu6P2pIlSmPawH6o24fogsfVySbDNQtKiOb7e5eiJ0pkqJRvRlMMjFMKwrynttD17X8DYMmTzxO4gjKKR4jTlK2Gd41NIF8wFl3dY3v3CqzunK4AS5TGtLHUK7HOGo6u2Oq1JwtEKTOUOJSEzKU4UpIYcepRUZxCdwa+GAHKkgxxIwx0KaXo6QrWA9fIpBOliHhgI/Ciqr5VRBYCXwFOBZ4H3qWqe4tjbwKuAwLwAVW9a5rjNuaEcEqEBXVWj1DJq5Snux5GYuQXOgRAtyScH2YhBOYiRFGiU4Ifmx1Uvmsj2Yt76MkS3NUvh/WvmrHfp1NNpUb5QeBxYG5x/0bgHlX9pIjcWNz/mIicA2wAzgVOBr4rImeqapjGuI05ISzUwIo6WzhIsQfOipDwtBtmq8vw0XMawuo077cMLl/hPIoiNRO8S09vhWdfJAmCnrQItUTZ0KTq3CKyEngL8Hc1xeuBW4vbtwLvqCm/TVVHVPU54Bng0mmJ1pgTTOqF4cQddonF+dzlmA/apM7jYomKh9QHkIASiKIkEcph7GwYkYxZWQUnI/le4KahydYo/wL4KDCnpmyZqm4HUNXtIlJd3WkF8OOa47YWZcaYCawEFtUpH1ThyTpjHiLw8r4+5i1fwBXAFZN8nSBloijq89MK1WmnDDy3VMNEKSJvBXaq6iYRec0knrPe+37YP7WIXA9cD7By5cpJPK0xx6+FE6Srx6Owq87PgkTueGjTpJ//9L/8Imv+8ovAWDMyUSFG6aT1c1tmMjXKK4C3i8ibgW5groh8AdghIsuL2uRyxmY3bAX6ah6/Etg2/klV9RbgFoC1a9d2zjwBY2ZQ6mGoTg71mi+xZmZGwz5KVb1JVVeq6qnkgzTfU9X3AHcC1xaHXQvcUdy+E9ggIl0ishpYAzww7ZEbcwJYGZXzQzzssjBalpxJxzKP8pPA7SJyHfAC8E4AVX1URG4HHgMy4AYb8Tbm6CzRgKsz6j0owr42OgM5O7mP8zbU62U9XGVxX+OD2syUEqWq3gvcW9zeA1wzwXE3AzcfY2zGnPAq3qF1TmPM2qxjcft3HmL/5skn7nKd5N/O7Mwc03TDKC9QZ0SvyTz52RBJB4/rJlGQOutILhdl9gTngdcz71WXEOb0Hlau5605pviqSnNgzprJZ2/fPS0vO2MsUZqmS4EtzHyiTMhHEjv5Q65C3RrlAmDBVL4ALnoZetHLpi+wcZIeSE5p2tO3XPt0chhjTJuyRGmMMQ1YojTGmAY6ufvGHAfedvEl9G7ZMq3POdDXxzencNaKMY1YojQt1btlC7ftmt5trzYsWdr4IGOmwJrexhjTgCVKY4xpwBKlMcY0YInSGGMasERpjDEN2Ki3aboycAp1TmEsClaKIkH4pc93CowSEMa2Pm3EAcvJz+2uOqUotw+4mQ72OTJN14VQb+mFauI8QxSJjudKSjlzhCQ7ZDOsRjxwOlCuSaxndvBCGKb9WNPbGGMasERpjDENWKI0xpgGLFEaY0wDNphjWiYrrrcrIIooRIkkCMuKce9agXyrz87aRMAcDyxRmpapFCt3P6kOPPgI0Snd6jiLw7dwGEbZw1iCNWamWNPbGGMasERpjDENWKI0xpgGLFEaY0wDNphj2k5U2I3iJKCxm+iHUUpUUKLmo961wzwK7AKSYjx8MbDjCGPjAiwCvJ3maCbJEqVpOxWERyUiUkHjXLRrFzFbQgQ8Su0EISGfNvR4zePPBX5+hOf3wGXArOkP3RynrOlt2o4iRO0i4FF3AAm76E6F3jTB6eG1SZtXaZrNEqVpOwL4mCDqqZR2cdBtZzgRhj2IgqgigKIoEMWSpWkuS5Sm7QiRRDNEE4I7SPADpKJoMoxDcaPVSEEFprAimzFHZVJ9lCLyPHCQvDsoU9V1IrIQ+ApwKvA88C5V3VscfxNwXXH8B1T1rmmP3By3VCJpMoTqLMox8sj3f8zgE8N0Jbs468o3cNKqVTiBSJ4krTZpmm0qgzlXqerumvs3Aveo6idF5Mbi/sdE5BxgA3mf+snAd0XkTFUN0xa1OS5UVyRfNK48AHtcJJIxK3r+7dYf8sg3v0bJ7eWDnzuZ5atWkIlDEUQPPyfcmOl2LKPe64HXFLdvBe4FPlaU36aqI8BzIvIMcClw/zG8ljkOdRXXa8eVD2vCT9Iu7v/hN9i2+d8Z3JmB28mIlvn3229j184tXHntb+J9N0mU0b5Ky5imWSbbR6nAd0Rkk4hcX5QtU9XtAMX10qJ8BbCl5rFbi7JDiMj1IrJRRDbu2bPn6KI3Ha1aF5Rx/6mAOvjFXT/mnz/1eZ589CkyL5SDsunbP+B7X7qD3hEoBYiiVLyiYg1w0zyTTZRXqOrFwJuAG0Tk1Uc4tt73+uH7SqneoqrrVHXdokXjG1/mRKYomQucdMZpnHP+xZSHPOWKA3UsP/NMTr/wfEKSAIIo+eCOMU00qaa3qm4rrneKyDfIm9I7RGS5qm4XkeXkSwVCXoPsq3n4SmDbNMZsjnNCPqDza9e/l8uvuZIPX/VWKpV+Br3wHz76e1z49rcw4Et0ZUIpKi7m/ZrW9DbN0jBRikgv4FT1YHH79cCfAHcC1wKfLK7vKB5yJ/AlEfk0+WDOGuCBJsRujlOCkqgSxFOZv4CL3vUO4sAgXiLzTukj9Z5YkxWVCGIz3UzzTKZGuQz4hohUj/+Sqn5bRB4EbheR64AXgHcCqOqjInI78Bj5Gqs32Ii3qScUPTL7xvXMjAA+KrNToTR/Ie/+85vpCoFFw5FePEOpI3OQqCCiZE7o1/rThOZyeEXTYxOIzdQ0TJSq+ixwYZ3yPcA1EzzmZuDmY47OHNeGi+uNdX8qo/OHeisCJIwkwkUUH9o4lhZTddwPpOOeIQEuYGx03ZijZYtimPYkY/XA0ZmSMu7+aOGRR3NspqU5VtYCMcaYBixRGmNMA5YojTGmAeujNC1T/ZaePa5cgYE6x0egn7GVzKvSYtK5OsUXP5qrku8VPq0RmxOVJUrTMt3F9aXjyoeBH5Mnxlop8FCd5xGgFIWspMzK8ploV2QJA12BJNppO+bYWaI0LVMdjXbj6n3CxDXBemkvilIpRyo+EmMsnlPpyiBa55KZBpYoTcdzCvMzJURlTpaXRSlW9x2/E5kxR8ESpel4CcL59OJRJMlrlKlXoliONNPDEqXpeAJ0xYzuUK1JgoqOrSpk2dIcI0uUpmOM5j0dXyoMlCNEcKqjx5QipNZHaaaBfYxMRwku/9B6LRbrFShHpRwiVFc6L1iSNNPFapSmY4hCOeTb1UYHwUWiwIBXCEnewq42s625baaRJUrTUapJ0mlkfgaeiBOtXUPDmGlnidJ0lMxB8Mq8VDk7mcPcNBIkkpC1OjRzHLNeHNNRnOYj20JkdhaZmwZmxYDtLWaayWqUpmMIUI5QUQgCAxKQUqS/HCgFOWSTsWFVHFACxNrl5hhZojQdQ4ERB0lwDInjITJwoCE/VbEU8hrnSuDHovQEWOuEcqsDNx3PEqXpHMWwdrV+OLrzogouQFQlKzqTBpOYn50T7CNujp19isxxoXY0HGBRJSI2Gm6miSVKc1wIDjKnzMnyjspLmE0gUrbRcDMNbNTbHBekGA13xSqWCyqR2cFGw830sERpOp4AXSFfxzIUQ9/DSaC/K4wukmHMsRDV1n+QRGQX+er/u1sdyziLab+YwOKaqnaMqx1jghM7rlWquqTeD9oiUQKIyEZVXdfqOGq1Y0xgcU1VO8bVjjGBxTURa3obY0wDliiNMaaBdkqUt7Q6gDraMSawuKaqHeNqx5jA4qqrbfoojTGmXbVTjdIYY9pSyxOliLxRRJ4UkWdE5MYZfu3Pi8hOEdlcU7ZQRO4WkaeL6wU1P7upiPNJEXlDk2LqE5Hvi8jjIvKoiHywTeLqFpEHROSRIq4/boe4itfxIvJTEflWu8RUvNbzIvJzEXlYRDa2Q2wiMl9EvioiTxSfscvbIKaziveoejkgIh9qdVyHUNWWXQAP/AI4DSgDjwDnzODrvxq4GNhcU/a/gBuL2zcCnypun1PE1wWsLuL2TYhpOXBxcXsO8FTx2q2OS4DZxe0S8BPgslbHVbzWR4AvAd9qh3/DmrieBxaPK2v1v+OtwPuK22VgfqtjGhefB34FrGqruJr55JN4Uy4H7qq5fxNw0wzHcCqHJsongeXF7eXAk/ViA+4CLp+B+O4AXtdOcQE9wEPAK1odF/mqavcAV9ckyrZ4ryZIlC2LDZgLPEcxNtEOMdWJ8fXAj9otrlY3vVcAW2ruby3KWmmZqm4HKK6XFuUzHquInApcRF57a3lcRRP3YWAncLeqtkNcfwF8FIqTvHOtjqlKge+IyCYRub4NYjsN2AX8Q9FV8Xci0tvimMbbAHy5uN02cbU6UdZbBKtdh+FnNFYRmQ18DfiQqh440qF1ypoSl6oGVV1LXou7VETOa2VcIvJWYKeqbprsQ+qUNfPzdoWqXgy8CbhBRF59hGNnIraEvKvpr1X1IvLTho80LjDTn/ky8HbgnxsdWqesqXmj1YlyK9BXc38lsK1FsVTtEJHlAMX1zqJ8xmIVkRJ5kvyiqn69XeKqUtV9wL3AG1sc1xXA20XkeeA24GoR+UKLYxqlqtuK653AN4BLWxzbVmBr0RIA+Cp54myL94v8C+UhVd1R3G+XuFqeKB8E1ojI6uLbZANwZ4tjuhO4trh9LXkfYbV8g4h0ichqYA3wwHS/uIgI8PfA46r66TaKa4mIzC9uzwJeCzzRyrhU9SZVXamqp5J/dr6nqu9pZUxVItIrInOqt8n73ja3MjZV/RWwRUTOKoquAR5rZUzjvJuxZnf19dshrtYO5hQdsW8mH9n9BfDxGX7tLwPbgZT8W+o6YBH54MDTxfXCmuM/XsT5JPCmJsX0SvJmxM+Ah4vLm9sgrguAnxZxbQb+R1He0rhqXus1jA3mtDwm8v7AR4rLo9XPdqtjA9YCG4t/x38BFrQ6puJ1eoA9wLyaspbHVb3YmTnGGNNAq5vexhjT9ixRGmNMA5YojTGmAUuUxhjTgCVKY4xpwBKlMcY0YInSGGMasERpjDEN/H8DNSjWsLqdrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "_idx = 9\n",
    "print(\"Target : \", annotations[_idx]['labels'])\n",
    "plot_image_from_output(imgs[_idx], annotations[_idx])\n",
    "print(\"Prediction : \", pred[_idx]['labels'])\n",
    "plot_image_from_output(imgs[_idx].cpu(), pred[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:11<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels = []\n",
    "preds_adj_all = []\n",
    "annot_all = []\n",
    "\n",
    "for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
    "    im = list(img.to(device) for img in im)\n",
    "    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
    "\n",
    "    for t in annot:\n",
    "        labels += t['labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_adj = make_prediction(model2, im, 0.5)\n",
    "        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "        preds_adj_all.append(preds_adj)\n",
    "        annot_all.append(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_batch_statistics(outputs, targets, iou_threshold):\n",
    "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
    "    batch_metrics = []\n",
    "    for sample_i in range(len(outputs)):\n",
    "\n",
    "        if outputs[sample_i] is None:\n",
    "            continue\n",
    "\n",
    "        output = outputs[sample_i] # predict\n",
    "        # pred_boxes = output['boxes']\n",
    "        # pred_scores = output['scores']\n",
    "        # pred_labels = output['labels']\n",
    "\n",
    "        true_positives = torch.zeros(output['boxes'].shape[0])   # 예측 객체 개수\n",
    " \n",
    "        annotations = targets[sample_i]  # actual\n",
    "        target_labels = annotations['labels'] if len(annotations) else []\n",
    "        if len(annotations):    # len(annotations) = 3\n",
    "            detected_boxes = []\n",
    "            target_boxes = annotations['boxes']\n",
    "\n",
    "            for pred_i, (pred_box, pred_label) in enumerate(zip(output['boxes'], output['labels'])): # 예측값에 대해서..\n",
    "\n",
    "                # If targets are found break\n",
    "                if len(detected_boxes) == len(target_labels): # annotations -> target_labels\n",
    "                    break\n",
    "\n",
    "                # Ignore if label is not one of the target labels\n",
    "                if pred_label not in target_labels:\n",
    "                    continue\n",
    "\n",
    "                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)   # box_index : 실제 어떤 바운딩 박스랑 IoU 가 가장 높은지 index\n",
    "                if iou >= iou_threshold and box_index not in detected_boxes: # iou만 맞으면 통과?\n",
    "                    true_positives[pred_i] = 1\n",
    "                    detected_boxes += [box_index]  # 예측된거랑 실제랑 매핑해서 하나씩 index 채움\n",
    "        batch_metrics.append([true_positives, output['scores'], output['labels']])\n",
    "    return batch_metrics\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (list).\n",
    "        conf:  Objectness value from 0-1 (list).\n",
    "        pred_cls: Predicted object classes (list).\n",
    "        target_cls: True object classes (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = torch.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = torch.unique(target_cls)   # 2가 거의 예측안됨\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    ap, p, r = [], [], []\n",
    "    for c in unique_classes:\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 and n_gt == 0:\n",
    "            continue\n",
    "        elif n_p == 0 or n_gt == 0:\n",
    "            ap.append(0)\n",
    "            r.append(0)\n",
    "            p.append(0)\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = torch.cumsum(1 - tp[i],-1)\n",
    "            tpc = torch.cumsum(tp[i],-1)\n",
    "\n",
    "            # Recall\n",
    "            recall_curve = tpc / (n_gt + 1e-16)\n",
    "            r.append(recall_curve[-1])\n",
    "\n",
    "            # Precision\n",
    "            precision_curve = tpc / (tpc + fpc)\n",
    "            p.append(precision_curve[-1])\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            ap.append(compute_ap(recall_curve, precision_curve))\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    p, r, ap = torch.tensor(np.array(p)), torch.tensor(np.array(r)), torch.tensor(np.array(ap))\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP : 0.0\n",
      "AP : tensor([0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "sample_metrics = []\n",
    "for batch_i in range(len(preds_adj_all)):\n",
    "    sample_metrics += get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "mAP = torch.mean(AP)\n",
    "print(f'mAP : {mAP}')\n",
    "print(f'AP : {AP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# # 모델 파라미터 저장\n",
    "# torch.save(model3.state_dict(), 'Pmodel_Bmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 저장된 모델 파라미터 파일 경로\n",
    "model_path = 'Pmodel_.pth'\n",
    "\n",
    "# 저장된 모델 파라미터 불러오기\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "# 이제 model 변수를 사용하여 예측을 수행하거나 추가 학습을 진행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 저장된 모델 파라미터 파일 경로\n",
    "model_path = 'Pmodel_Bmodel.pth'\n",
    "\n",
    "# 저장된 모델 파라미터 불러오기\n",
    "model1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "\n",
    "# 이제 model 변수를 사용하여 예측을 수행하거나 추가 학습을 진행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# model1과 model을 불러옵니다.\n",
    "\n",
    "\n",
    "# detector 파라미터 추출\n",
    "model1_detector_params = model1.roi_heads.state_dict()\n",
    "model_detector_params = model.roi_heads.state_dict()\n",
    "\n",
    "# detector 파라미터 평균 계산\n",
    "ensemble_detector_params = {}\n",
    "for key in model1_detector_params:\n",
    "    ensemble_detector_params[key] = (model1_detector_params[key] + model_detector_params[key]) / 2\n",
    "\n",
    "# 새로운 앙상블 모델을 정의하고 detector 파라미터 적용\n",
    "model3.roi_heads.load_state_dict(ensemble_detector_params)\n",
    "\n",
    "# 이제 ensemble_model을 사용하여 앙상블 검출을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1에서 백본 모델 파라미터 가져오기\n",
    "backbone_params = model3.roi_heads.state_dict()\n",
    "\n",
    "# model의 백본 파라미터에 복사\n",
    "model2.roi_heads.load_state_dict(backbone_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1에서 백본 모델 파라미터 가져오기\n",
    "backbone_params = model1.roi_heads.state_dict()\n",
    "\n",
    "# model의 백본 파라미터에 복사\n",
    "model.roi_heads.load_state_dict(backbone_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
